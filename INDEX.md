"""
PROJECT INDEX & NAVIGATION GUIDE
Stock Price Prediction - Machine Learning System
"""

# ============================================================================
# STOCK PRICE PREDICTION PROJECT - COMPLETE FILE INDEX
# ============================================================================

# QUICK NAVIGATION
# ================

## üëâ START HERE
1. Read: PROJECT_SUMMARY.md        (5 min) - Overview of entire project
2. Read: SETUP.md                  (10 min) - Installation instructions
3. Run:  python quickstart.py      (5-10 min) - Execute pipeline
4. Explore: streamlit run dashboard.py - Interactive dashboard

## üìñ FOR UNDERSTANDING
- README.md                         - Complete documentation & examples
- config.py                         - Customizable settings
- notebooks/complete_example.py     - Step-by-step walkthrough


# DETAILED FILE GUIDE
# ===================

## CORE PIPELINE MODULES (Data Processing & ML)

### üì• data_loader.py
**Purpose**: Download and validate stock market data
**Key Class**: StockDataLoader
**Key Methods**:
  - fetch_data() ‚Üí Download from Yahoo Finance
  - load_from_csv() ‚Üí Load local CSV files
  - validate_data() ‚Üí Check data quality
  - save_to_csv() ‚Üí Export data

**When to Use**:
  - Getting stock price data
  - Validating data quality
  - Loading custom datasets

**Example**:
  from data_loader import StockDataLoader
  loader = StockDataLoader('AAPL')
  data = loader.fetch_data()


### üõ†Ô∏è feature_engineering.py
**Purpose**: Create 50+ technical indicators and derived features
**Key Class**: FeatureEngineer
**Key Methods**:
  - add_moving_averages() ‚Üí SMA, EMA
  - add_momentum_indicators() ‚Üí RSI, MACD, Stochastic
  - add_volatility_indicators() ‚Üí Bollinger Bands, ATR
  - add_volume_indicators() ‚Üí OBV, CMF
  - add_price_features() ‚Üí Returns, ratios
  - add_time_features() ‚Üí Date-based features
  - engineer_all_features() ‚Üí All features at once

**Features Created**:
  ‚Ä¢ Moving Averages (8) - SMA & EMA for 4 periods
  ‚Ä¢ Momentum (8) - RSI, MACD, Stochastic
  ‚Ä¢ Volatility (5) - Bollinger Bands, ATR, Volatility
  ‚Ä¢ Volume (3) - OBV, CMF, Volume SMA
  ‚Ä¢ Price (5) - Returns, ratios, highs/lows
  ‚Ä¢ Time (10+) - Day, month, quarter, encoded features
  ‚Ä¢ Lag (12) - Previous 1, 3, 5, 10 day values
  ‚Ä¢ Rolling (12) - Rolling statistics

**When to Use**:
  - Creating features for ML models
  - Technical analysis
  - Feature visualization

**Example**:
  from feature_engineering import FeatureEngineer
  engineer = FeatureEngineer(data)
  features = engineer.engineer_all_features()


### ü§ñ model_training.py
**Purpose**: Implement multiple machine learning models
**Key Classes**:
  - LinearRegressionModel - Fast baseline
  - RandomForestModel - Feature importance
  - XGBoostModel - Best accuracy
  - LightGBMModel - Fast, memory efficient
  - LSTMModel - Neural network for sequences
  - ModelEnsemble - Combine multiple models

**Methods** (common to all):
  - fit(X_train, y_train) ‚Üí Train model
  - predict(X_test) ‚Üí Make predictions
  - evaluate(y_true, y_pred) ‚Üí Calculate metrics
  - save_model(filepath) ‚Üí Save to disk
  - load_model(filepath) ‚Üí Load from disk

**When to Use**:
  - Training individual models
  - Creating ensembles
  - Model comparison

**Example**:
  from model_training import XGBoostModel
  model = XGBoostModel()
  model.fit(X_train, y_train)
  predictions = model.predict(X_test)


### üîÑ main_pipeline.py
**Purpose**: Orchestrate entire ML pipeline from start to finish
**Key Class**: StockPricePredictionPipeline
**Key Methods**:
  - load_data() ‚Üí Step 1: Get data
  - engineer_features() ‚Üí Step 2: Create features
  - prepare_data() ‚Üí Step 3: Split & scale
  - train_models() ‚Üí Step 4: Train all models
  - evaluate_models() ‚Üí Step 5: Evaluate performance
  - plot_predictions() ‚Üí Step 6: Visualize results
  - run_pipeline() ‚Üí Run all steps

**Features**:
  ‚úì Automated data loading and validation
  ‚úì Feature engineering pipeline
  ‚úì Time-series aware train/val/test split
  ‚úì Feature scaling (StandardScaler)
  ‚úì Train 5 models simultaneously
  ‚úì Comprehensive evaluation metrics
  ‚úì Auto-generate visualizations

**When to Use**:
  - Running complete analysis
  - Comparing different stocks
  - Batch processing multiple tickers

**Example**:
  from main_pipeline import StockPricePredictionPipeline
  pipeline = StockPricePredictionPipeline('AAPL')
  pipeline.run_pipeline()


## USER INTERFACES (Interaction & Visualization)

### üé® dashboard.py
**Purpose**: Interactive web dashboard using Streamlit
**Components**: 5 tabs for complete workflow
  1. üìà Dashboard - Overview & price charts
  2. üî¨ Data Analysis - Load, engineer, explore
  3. ü§ñ Model Training - Train models interactively
  4. üìä Results - View performance metrics
  5. üí° Predictions - See predictions

**Features**:
  ‚úì Interactive configuration
  ‚úì Real-time data loading
  ‚úì Visual feature engineering
  ‚úì Model selection UI
  ‚úì Performance comparison charts
  ‚úì Prediction visualization

**How to Run**:
  streamlit run dashboard.py
  # Opens http://localhost:8501

**Best For**:
  - Non-technical users
  - Interactive exploration
  - Quick experimentation
  - Presentations


### üöÄ quickstart.py
**Purpose**: CLI interface for rapid pipeline execution
**Features**:
  ‚úì Simple command-line prompts
  ‚úì Auto-configuration
  ‚úì Progress tracking
  ‚úì Summary output

**How to Run**:
  python quickstart.py
  # Follow interactive prompts

**Best For**:
  - Quick testing
  - Automated scripts
  - CI/CD integration
  - Batch processing


## CONFIGURATION & UTILITIES

### ‚öôÔ∏è config.py
**Purpose**: Centralized configuration management
**Sections**:
  ‚Ä¢ DATA_CONFIG - Ticker, date range
  ‚Ä¢ FEATURE_CONFIG - Indicator periods, future days
  ‚Ä¢ SPLIT_CONFIG - Train/val/test proportions
  ‚Ä¢ SCALING_CONFIG - Scaling method
  ‚Ä¢ MODEL_CONFIG - Hyperparameters for each model
  ‚Ä¢ ENSEMBLE_CONFIG - Ensemble weights
  ‚Ä¢ EVALUATION_CONFIG - Metrics and plots
  ‚Ä¢ DIRS_CONFIG - Directory paths
  ‚Ä¢ LOGGING_CONFIG - Logging settings
  ‚Ä¢ API_CONFIG - External API keys
  ‚Ä¢ ADVANCED_CONFIG - Feature selection, outliers, etc.
  ‚Ä¢ TUNING_CONFIG - Hyperparameter tuning
  ‚Ä¢ BACKTEST_CONFIG - Backtesting settings

**How to Customize**:
  1. Edit config.py values
  2. Run pipeline
  3. Settings automatically applied

**Example**:
  from config import get_config
  ticker = get_config('data', 'default_ticker')
  ma_periods = get_config('features', 'ma_periods')


### üîß utils.py
**Purpose**: Helper utilities and analysis tools
**Classes**:
  - DataValidator - Check data quality
  - MetricsCalculator - Additional metrics
  - FileManager - File operations
  - TimeSeriesHelper - Time series utilities
  - BacktestHelper - Backtesting metrics
  - LoggingHelper - Logging setup
  - PerformanceAnalyzer - Model analysis

**Common Functions**:
  ‚Ä¢ DataValidator.check_data_quality()
  ‚Ä¢ MetricsCalculator.calculate_directional_accuracy()
  ‚Ä¢ TimeSeriesHelper.get_trading_days()
  ‚Ä¢ FileManager.ensure_directory()
  ‚Ä¢ BacktestHelper.calculate_sharpe_ratio()

**When to Use**:
  - Data quality checking
  - Additional metrics
  - Backtesting analysis
  - Utility operations


## DOCUMENTATION & EXAMPLES

### üìñ README.md (MAIN DOCUMENTATION)
**Contents**:
  1. Project overview
  2. Quick start guide
  3. Project structure
  4. Feature descriptions
  5. Model documentation
  6. Evaluation metrics
  7. Usage examples
  8. Configuration guide
  9. Dashboard features
  10. Technical stack
  11. Important considerations
  12. References

**Use When**:
  - Need comprehensive documentation
  - Want to understand all features
  - Looking for usage examples
  - Need references

### üìã SETUP.md (INSTALLATION GUIDE)
**Contents**:
  1. Prerequisites
  2. Virtual environment setup
  3. Dependency installation
  4. Directory structure
  5. Three quick-start options
  6. Customization guide
  7. Troubleshooting
  8. Custom data setup
  9. Advanced configuration
  10. Deployment guide

**Use When**:
  - Installing project first time
  - Troubleshooting setup issues
  - Setting up production environment
  - Deploying to cloud

### üìä PROJECT_SUMMARY.md
**Contents**:
  1. Quick overview
  2. Structure summary
  3. Start options
  4. Data pipeline diagram
  5. Model descriptions
  6. Feature engineering summary
  7. Evaluation metrics
  8. Output files
  9. Dashboard features
  10. Customization options
  11. Code organization
  12. Learning outcomes
  13. Future improvements

**Use When**:
  - Need quick overview
  - Want project summary
  - Planning improvements
  - Learning about ML pipeline

### üéì notebooks/complete_example.py
**Contents**:
  15 cells with complete walkthrough:
  1. Library imports
  2. Data loading
  3. EDA
  4. Feature engineering
  5. Data preparation
  6. Linear Regression
  7. Random Forest
  8. XGBoost
  9. LightGBM
  10. LSTM
  11. Model comparison
  12. Visualizations
  13. Feature importance
  14. Analysis summary
  15. Results

**Use When**:
  - Learning step-by-step
  - Understanding each component
  - Modifying individual steps
  - Educational purposes

**Run As**:
  python notebooks/complete_example.py


## DATA & OUTPUT FILES

### üìÅ data/
**Contents**: Historical stock data
**Files Created**:
  - AAPL_historical_data.csv (after running pipeline)
  - Any custom CSV files you load

**Format**:
  date, open, high, low, close, volume
  2023-01-01, 100.5, 101.2, 99.8, 100.8, 1000000

### üìÅ models/
**Contents**: Trained model files
**Files Created**:
  - Linear_Regression_model.pkl
  - Random_Forest_model.pkl
  - XGBoost_model.pkl
  - LightGBM_model.pkl
  - LSTM_model.pkl

**Usage**:
  import joblib
  model = joblib.load('models/XGBoost_model.pkl')
  predictions = model.predict(X_test)

### üìÅ visualizations/
**Contents**: Generated plots
**Files Created**:
  - predictions_comparison.png (4-model comparison)
  - ensemble_predictions.png (Best model)
  - feature_importance.png (Top features)
  - model_comparison.png (Metrics comparison)


## REQUIREMENTS & DEPENDENCIES

### requirements.txt
**Python Packages**:
  - pandas, numpy - Data processing
  - scikit-learn - Machine learning
  - tensorflow, keras - Deep learning
  - torch - PyTorch (alternative to TF)
  - xgboost - Gradient boosting
  - lightgbm - Light gradient boosting
  - yfinance - Stock data
  - ta - Technical analysis
  - matplotlib, seaborn - Static plots
  - plotly - Interactive plots
  - streamlit - Web dashboard
  - joblib - Model serialization

**Install**:
  pip install -r requirements.txt


## WORKFLOW DIAGRAMS

### Data Flow Pipeline
```
Yahoo Finance
     ‚Üì
DataLoader (fetch_data)
     ‚Üì
Data Validation
     ‚Üì
FeatureEngineer (engineer_all_features)
     ‚Üì
Feature Scaling (StandardScaler)
     ‚Üì
Train/Val/Test Split (Time-Series Aware)
     ‚Üì
Model Training (5 models in parallel)
     ‚Üì
Evaluation (RMSE, MAE, R¬≤)
     ‚Üì
Visualization & Output
```

### File Dependency Graph
```
data_loader.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îú‚îÄ‚îÄ‚Üí main_pipeline.py ‚îÄ‚îÄ‚Üí dashboard.py
feature_engineering.py ‚îÄ‚î§
                        ‚îú‚îÄ‚îÄ‚Üí quickstart.py
model_training.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                        ‚îî‚îÄ‚îÄ‚Üí notebook examples
        ‚Üì
    config.py, utils.py
```


## TYPICAL WORKFLOWS

### Workflow 1: Quick Test (5-10 min)
```
python quickstart.py
‚Üí Follow prompts (press Enter for defaults)
‚Üí View results in visualizations/
```

### Workflow 2: Interactive Dashboard (10-20 min)
```
streamlit run dashboard.py
‚Üí Tab 1: Load Data
‚Üí Tab 2: Engineer Features
‚Üí Tab 3: Prepare & Train
‚Üí Tab 4: View Results
‚Üí Tab 5: Make Predictions
```

### Workflow 3: Custom Script (Flexible)
```python
from main_pipeline import StockPricePredictionPipeline

pipeline = StockPricePredictionPipeline('MSFT')
pipeline.load_data()
pipeline.engineer_features(future_days=10)
pipeline.prepare_data(test_size=0.15)
pipeline.train_models()
pipeline.evaluate_models()
```

### Workflow 4: Learning & Experimentation
```
1. Read: README.md & PROJECT_SUMMARY.md
2. Run: notebooks/complete_example.py
3. Study: Each module's code comments
4. Modify: config.py for different settings
5. Explore: dashboard.py for visualization
```


## CUSTOMIZATION EXAMPLES

### Example 1: Different Stock
```python
from main_pipeline import StockPricePredictionPipeline
pipeline = StockPricePredictionPipeline('GOOGL')
pipeline.run_pipeline()
```

### Example 2: Different Time Horizon
```python
pipeline = StockPricePredictionPipeline('AAPL')
pipeline.run_pipeline(future_days=10)  # Predict 10 days ahead
```

### Example 3: Different Models Only
```python
from main_pipeline import StockPricePredictionPipeline
pipeline = StockPricePredictionPipeline('AAPL')
pipeline.load_data()
pipeline.engineer_features()
pipeline.prepare_data()

# Only train specific models
from model_training import XGBoostModel, LightGBMModel
pipeline.models = {
    'XGBoost': XGBoostModel(),
    'LightGBM': LightGBMModel(),
}
pipeline.train_models()
pipeline.evaluate_models()
```

### Example 4: Use Custom Data
```python
from main_pipeline import StockPricePredictionPipeline
pipeline = StockPricePredictionPipeline('CUSTOM')
pipeline.load_data(use_csv='path/to/your/data.csv')
pipeline.engineer_features()
pipeline.prepare_data()
pipeline.train_models()
```


## TROUBLESHOOTING MATRIX

| Problem | Check File | Solution |
|---------|-----------|----------|
| Import error | requirements.txt | pip install -r requirements.txt |
| Data download fails | data_loader.py | Check internet, use CSV |
| Feature error | feature_engineering.py | Ensure data is valid |
| Model training slow | config.py | Reduce n_estimators |
| Memory error | main_pipeline.py | Use smaller dataset |
| Dashboard not launching | dashboard.py | streamlit run dashboard.py |
| Configuration issues | config.py | Review config values |
| Visualization missing | main_pipeline.py | Check visualizations/ directory |


## KEY TAKEAWAYS

‚úì **Complete ML Pipeline**: Everything from data to predictions
‚úì **Multiple Models**: Compare 5 different algorithms
‚úì **Feature Engineering**: 50+ technical indicators
‚úì **Easy to Use**: 3 ways to interact (CLI, Dashboard, Python)
‚úì **Well Documented**: README, SETUP, examples, comments
‚úì **Extensible**: Easy to modify and customize
‚úì **Production Ready**: Proper error handling, validation, logging


## NEXT STEPS

1. **First Time Users**:
   - Read PROJECT_SUMMARY.md
   - Follow SETUP.md
   - Run: python quickstart.py
   - Explore: streamlit run dashboard.py

2. **Learners**:
   - Read: README.md
   - Review: notebooks/complete_example.py
   - Study: Source code comments
   - Modify: config.py settings

3. **Advanced Users**:
   - Customize: All modules
   - Add: New features/models
   - Deploy: Production systems
   - Integrate: Additional data sources


---

**Happy Learning & Analyzing! üìäüöÄ**

For more information, see the detailed documentation files.
